# Prompt 缓存 (Cached Tokens)

> 本文总结了一次围绕 **LLM 缓存 token、Agent Prompt 设计、组合 Agent 架构** 的完整技术讨论，目标是建立**正确、可落地的工程认知**，而不是停留在表面“省钱技巧”。

---

## 一、什么是 LLM 的缓存 Token（Cached Tokens）

**一句话定义：**

> 缓存 token = **模型已经计算过、可以复用中间结果的输入 token**，再次使用时计费更低。

它不是：
- ❌ 记忆
- ❌ 数据库缓存
- ❌ 对话历史

而是：
- ✅ **模型前向计算（Attention / KV Cache）的复用**
- ✅ 纯粹的 **数学计算结果缓存**

---

## 二、缓存 Token 的判定条件（非常严格）

是否命中缓存，仅取决于：

```text
同一个模型 + 完全一致的输入 token 前缀
```

**与以下因素无关：**

* API Key
* 用户是谁
* 地理位置
* 应用实例
* 是否是同一个 Agent
* 是否是同一个进程或服务器

---

## 三、一个容易误解但必须澄清的问题

### ❓ 不同用户会不会“用到彼此的缓存”？

**从计算复用层面：是的。
从数据与安全层面：完全不是你以为的那种“共享”。**

关键澄清：

> **缓存的不是“文本内容”，
> 而是“模型对这段 token 的数学中间状态”。**

只有在 **token 序列 100% 一致** 的情况下，模型才能安全复用这些中间结果。

* 不会泄露 prompt
* 不会暴露用户
* 不会共享输出
* 也无法被反向推断

---

## 四、为什么 Agent 应用天然适合吃缓存？

Agent 的 Prompt 结构通常是：

```text
System Prompt（很长、很稳定）
+ Tool Schema（很长、很稳定）
+ 行为规则（很长、很稳定）
--------------------------------
User Input（短、变化）
```

这意味着：

* 前缀高度稳定
* 重复率极高
* 成本中最大的一段可以被缓存

📌 **缓存机制本质上“奖励”了工程化 Prompt。**

---

## 五、组合 Agent 是如何“各自命中缓存”的？

### 关键结论

> **不是人为划分缓存，也不是靠不同 API Key，
> 而是每个 Agent 都有自己长期稳定、重复出现的 Prompt 前缀。**

例如：

* Planner Agent：任务拆解规则
* Executor Agent：工具调用协议
* Critic Agent：结果校验规则

它们的 system prompt 不同 → token 前缀不同 →
**自然形成各自的缓存命中模式**。

📌 这是统计意义上的“高频前缀复用”，不是显式配置。

---

## 六、真正导致缓存失效的工程雷区（重点）

### 1️⃣ Prompt 中混入 runtime 信息

```text
当前时间是 {{now}}
request_id: {{uuid}}
```

👉 每次都不同 → 缓存 0% 命中

---

### 2️⃣ Tool Schema / JSON 顺序不稳定（Node 非常常见）

```js
JSON.stringify(schema) // key 顺序可能变化
```

👉 token 改变 = cache miss

---

### 3️⃣ Agent 框架偷偷改 system prompt

* 自动加说明
* 自动拼历史
* 自动注入调试信息

👉 “你以为没变，其实已经变了”

---

### 4️⃣ 多语言 / 多模板混用

* 有时中文
* 有时英文

👉 token 完全不同，缓存完全不同

---

### 5️⃣ 一个 Agent 多种人格 Prompt

```text
Executor-fast
Executor-safe
Executor-v2
```

👉 本质是多个 Agent，缓存不会共享

---

## 七、Agent 工程的核心心法（比缓存更重要）

### 1️⃣ Prompt = 程序，不是说明书

Agent 更像：

* 状态机
* 控制器
* 协议解释器

而不是聊天机器人。

---

### 2️⃣ 判断 / 分支 / 中止条件必须前置

一个成熟的 Agent Prompt，必须明确：

* 何时调用工具
* 何时重试
* 何时失败
* 何时停止

---

### 3️⃣ Tool 设计 > Prompt 设计

高质量 Agent 依赖：

* 严格 schema
* 稳定输出
* 可枚举错误

---

### 4️⃣ 不要迷信“超长上下文”

更成熟的做法是：

* LLM 负责决策
* 状态存在外部（DB / KV / Vector）
* Prompt 只保留“最小决策上下文”

---

### 5️⃣ 组合 Agent > 单一万能 Agent

拆分为：

* Planner
* Executor
* Critic / Validator

不仅更稳定，也更容易命中缓存。

---

## 八、最终正确的心智模型（请记住）

> 1. 缓存的是 **计算，不是文本**
> 2. 判定标准只有 **token 是否完全一致**
> 3. 与用户、地区、API Key 无关
> 4. 稳定的 Agent Prompt = 高缓存命中率
> 5. 好 Agent 本质上是“可预测的纯函数”

---

## 九、下一期你想看什么？

你更希望深入哪一块？

* 🔹 RAG 的工程设计与常见误区
* 🔹 组合 Agent 之间的调用与编排模式
* 🔹 Agent Tool / MCP 的协议设计最佳实践
* 🔹 如何调试、测试、评估一个 Agent
* 🔹 长期运行 Agent 的状态与记忆管理
* 🔹 单 Agent vs 多 Agent 的真实成本对比

👉 可以直接选编号，或提出你更关心的方向。

